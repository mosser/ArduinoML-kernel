"use strict";
/******************************************************************************
 * Copyright 2021 TypeFox GmbH
 * This program and the accompanying materials are made available under the
 * terms of the MIT License, which is available in the project root.
 ******************************************************************************/
Object.defineProperty(exports, "__esModule", { value: true });
exports.DefaultTokenBuilder = void 0;
const chevrotain_1 = require("chevrotain");
const ast_1 = require("../grammar/generated/ast");
const internal_grammar_util_1 = require("../grammar/internal-grammar-util");
const ast_util_1 = require("../utils/ast-util");
const grammar_util_1 = require("../utils/grammar-util");
const regex_util_1 = require("../utils/regex-util");
const stream_1 = require("../utils/stream");
class DefaultTokenBuilder {
    buildTokens(grammar, options) {
        const reachableRules = (0, stream_1.stream)((0, grammar_util_1.getAllReachableRules)(grammar, false));
        const terminalTokens = this.buildTerminalTokens(reachableRules);
        const tokens = this.buildKeywordTokens(reachableRules, terminalTokens, options);
        terminalTokens.forEach(terminalToken => {
            const pattern = terminalToken.PATTERN;
            if (typeof pattern === 'object' && pattern && 'test' in pattern && (0, regex_util_1.isWhitespaceRegExp)(pattern)) {
                tokens.unshift(terminalToken);
            }
            else {
                tokens.push(terminalToken);
            }
        });
        return tokens;
    }
    buildTerminalTokens(rules) {
        return rules.filter(ast_1.isTerminalRule).filter(e => !e.fragment)
            .map(terminal => this.buildTerminalToken(terminal)).toArray();
    }
    buildTerminalToken(terminal) {
        const regex = (0, internal_grammar_util_1.terminalRegex)(terminal);
        const token = { name: terminal.name, PATTERN: new RegExp(regex) };
        if (terminal.hidden) {
            // Only skip tokens that are able to accept whitespace
            token.GROUP = (0, regex_util_1.isWhitespaceRegExp)(regex) ? chevrotain_1.Lexer.SKIPPED : 'hidden';
        }
        return token;
    }
    buildKeywordTokens(rules, terminalTokens, options) {
        return rules
            // We filter by parser rules, since keywords in terminal rules get transformed into regex and are not actual tokens
            .filter(ast_1.isParserRule)
            .flatMap(rule => (0, ast_util_1.streamAllContents)(rule).filter(ast_1.isKeyword))
            .distinct(e => e.value).toArray()
            // Sort keywords by descending length
            .sort((a, b) => b.value.length - a.value.length)
            .map(keyword => this.buildKeywordToken(keyword, terminalTokens, Boolean(options === null || options === void 0 ? void 0 : options.caseInsensitive)));
    }
    buildKeywordToken(keyword, terminalTokens, caseInsensitive) {
        return {
            name: keyword.value,
            PATTERN: this.buildKeywordPattern(keyword, caseInsensitive),
            LONGER_ALT: this.findLongerAlt(keyword, terminalTokens)
        };
    }
    buildKeywordPattern(keyword, caseInsensitive) {
        return caseInsensitive ?
            new RegExp((0, regex_util_1.getCaseInsensitivePattern)(keyword.value)) :
            keyword.value;
    }
    findLongerAlt(keyword, terminalTokens) {
        return terminalTokens.reduce((longerAlts, token) => {
            const pattern = token === null || token === void 0 ? void 0 : token.PATTERN;
            if ((pattern === null || pattern === void 0 ? void 0 : pattern.source) && (0, regex_util_1.partialMatches)('^' + pattern.source + '$', keyword.value)) {
                longerAlts.push(token);
            }
            return longerAlts;
        }, []);
    }
}
exports.DefaultTokenBuilder = DefaultTokenBuilder;
//# sourceMappingURL=token-builder.js.map